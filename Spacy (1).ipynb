{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53eabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab3d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #faster compared to nltk when we are working with lager projects can work on diff languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a686c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaa51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')   #to load the model that has diff aspects of english language including grammar etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp('Gfg is looking for ds interns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba810432",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b6fd4",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e281ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text:\n",
    "    print(type(token))  #token from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901086eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert to str\n",
    "for token in text:\n",
    "    print(type(token.text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74226786",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =nlp('the cost of Iphone in U.K is 699$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853df07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in txt:\n",
    "    print(token)  #although 699$ doesnot have space they are treated as diff token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d6b27",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in txt:\n",
    "    print(token.text ,token.pos_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in txt:\n",
    "    print(token.text,\"  \",token.pos) #codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e84e6",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97af772",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = nlp('this is sen1. this is sen2. this is sen3. lets study now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ef8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in txt:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88747e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in txt.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa267c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5977fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize('this is sen1. this is sen2. this is sen3. lets study now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize('this is sen1. this is sen2. this is sen3. lets study now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "'this is sen1. this is sen2. this is sen3. lets study now'.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bd799",
   "metadata": {},
   "source": [
    "## Stop-Words Removal with Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7494c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e08f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    " nlp = spacy.load('en_core_web_sm') #sm =smaller lg=larger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0173e77",
   "metadata": {},
   "source": [
    "## print all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd953cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp.vocab['is'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp.Defaults.stop_words) ## bcoz it contains all unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b01560",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d52f4",
   "metadata": {},
   "source": [
    "## to check if a word is stopword or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp.vocab['the'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfe4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['hello'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3e7e0",
   "metadata": {},
   "source": [
    "## Adding custom  words to list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('i.e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551baeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e321f",
   "metadata": {},
   "source": [
    "## Removing custom words to list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79288675",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('i.e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f79054",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bdab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['i.e'].is_stop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b4854",
   "metadata": {},
   "source": [
    "## removing stop words from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '''Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data. This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.\n",
    "Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information. Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ae612",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.replace('\\n','')\n",
    "t= t.strip() #removes extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad33cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = nlp(t) #pass text t through modelnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d844a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in corp:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in corp:\n",
    "    if tok.is_stop:\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd1660",
   "metadata": {},
   "source": [
    "## finding stopwords in corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word= []    #list\n",
    "for tok in corp:\n",
    "    if tok.is_stop:\n",
    "        stop_word.append(tok)\n",
    "        \n",
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a902513",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word= set()   #set\n",
    "for tok in corp:\n",
    "    if tok.is_stop:\n",
    "        stop_word.add(tok.text)   #tok.text = string\n",
    "          \n",
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab74d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stop_word)) #length of unique stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163eac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97035037",
   "metadata": {},
   "source": [
    "## Finding words that do not belong to stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in corp:\n",
    "    if not token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list comprehension\n",
    "print([token for token in corp if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([token.text for token in corp if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203ad10",
   "metadata": {},
   "source": [
    "## Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f14b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet  #wordnet=dict everything abt english language grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cae50fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e87efe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01'),\n",
       " Synset('book.n.02'),\n",
       " Synset('record.n.05'),\n",
       " Synset('script.n.01'),\n",
       " Synset('ledger.n.01'),\n",
       " Synset('book.n.06'),\n",
       " Synset('book.n.07'),\n",
       " Synset('koran.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('book.n.10'),\n",
       " Synset('book.n.11'),\n",
       " Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('book')  #synsets= synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "784bc4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('car')[0]\n",
    "#to get only 1st synonym \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7af6d120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01'),\n",
       " Synset('book.n.02'),\n",
       " Synset('record.n.05'),\n",
       " Synset('script.n.01'),\n",
       " Synset('ledger.n.01'),\n",
       " Synset('book.n.06'),\n",
       " Synset('book.n.07'),\n",
       " Synset('koran.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('book.n.10'),\n",
       " Synset('book.n.11'),\n",
       " Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s=wordnet.synsets('Book')\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8899d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('book.n.01')\n",
      "a written work or composition that has been published (printed on pages bound together)\n"
     ]
    }
   ],
   "source": [
    " #gives the exact definition of a word at particular index\n",
    "print(s[0])\n",
    "print(s[0].definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d14a6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('script.n.01')\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance\n"
     ]
    }
   ],
   "source": [
    "print(s[3])\n",
    "print(s[3].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179de08",
   "metadata": {},
   "source": [
    "##  printing synonyms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028730eb",
   "metadata": {},
   "source": [
    "\n",
    "1. **Lemma:**\n",
    "   - A lemma is the canonical form of a word. For example, \"run\" is the lemma for \"running\", \"ran\", and \"runs\".\n",
    "   - In the code, `lemma.name()` retrieves the canonical form (lemma) of each synonym for the word 'happy'.\n",
    "\n",
    "2. **Syn:**\n",
    "   - Short for synset, which stands for \"synonym set\".\n",
    "   - A synset is a set of synonyms that share a common meaning. For instance, a synset for 'happy' might include 'joyful', 'content', and 'cheerful'.\n",
    "   - In the code, `wordnet.synsets('happy')` retrieves all synsets for the word 'happy'. Each synset contains multiple lemmas that are synonyms of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb6c5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cb6b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      "[Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "[Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "[Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "[Lemma('car.n.02.car'), Lemma('car.n.02.railcar'), Lemma('car.n.02.railway_car'), Lemma('car.n.02.railroad_car')]\n",
      "[Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
      "[Lemma('car.n.03.car'), Lemma('car.n.03.gondola')]\n",
      "[Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
      "[Lemma('car.n.04.car'), Lemma('car.n.04.elevator_car')]\n",
      "[Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n",
      "[Lemma('cable_car.n.01.cable_car'), Lemma('cable_car.n.01.car')]\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('car'):\n",
    "    for lemma in syn.lemmas():\n",
    "         print(syn.lemmas())  #detailed version of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b24f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "auto\n",
      "automobile\n",
      "machine\n",
      "motorcar\n",
      "car\n",
      "railcar\n",
      "railway_car\n",
      "railroad_car\n",
      "car\n",
      "gondola\n",
      "car\n",
      "elevator_car\n",
      "cable_car\n",
      "car\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('car'):\n",
    "    for lemma in syn.lemmas():\n",
    "        print(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4e1762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'felicitous', 'happy', 'glad', 'happy', 'happy', 'well-chosen']\n"
     ]
    }
   ],
   "source": [
    "#to print in list format\n",
    "synonmys = []\n",
    "for syn in wordnet.synsets('happy'): #Iterating over each synset for the word 'happy':\n",
    "    for lemma in syn.lemmas():    #Iterating over each lemma in the current synset:\n",
    "        synonmys.append(lemma.name())\n",
    "print(synonmys)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111a474",
   "metadata": {},
   "source": [
    "## printing antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f90d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('unhappy.a.01.unhappy')]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "for ant in wordnet.synsets('happy'):   #synset because for every syn it genrates antonym\n",
    "    for lemma in ant.lemmas():\n",
    "        print(lemma.antonyms())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c05f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('happy.a.01.happy') [Lemma('unhappy.a.01.unhappy')]\n",
      "Lemma('felicitous.s.02.felicitous') []\n",
      "Lemma('felicitous.s.02.happy') []\n",
      "Lemma('glad.s.02.glad') []\n",
      "Lemma('glad.s.02.happy') []\n",
      "Lemma('happy.s.04.happy') []\n",
      "Lemma('happy.s.04.well-chosen') []\n"
     ]
    }
   ],
   "source": [
    "#ex1\n",
    "antonyms = []\n",
    "for ant in wordnet.synsets('happy'):   #synset because for every syn it genrates antonym\n",
    "    for lemma in ant.lemmas():\n",
    "        print(lemma , lemma.antonyms())\n",
    "        #few are empty ie its not genrating any antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ef78e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('happy.a.01.happy') [Lemma('unhappy.a.01.unhappy')]\n"
     ]
    }
   ],
   "source": [
    "#to print only the values that arent null \n",
    "antonyms = []\n",
    "for ant in wordnet.synsets('happy'):   #synset because for every syn it genrates antonym\n",
    "    for lemma in ant.lemmas():\n",
    "        if lemma.antonyms():  #if it has value then print \n",
    "            print(lemma , lemma.antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef118bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('good.n.01.good') []\n",
      "Lemma('good.n.02.good') [Lemma('evil.n.03.evil')]\n",
      "Lemma('good.n.02.goodness') [Lemma('evil.n.03.evilness')]\n",
      "Lemma('good.n.03.good') [Lemma('bad.n.01.bad')]\n",
      "Lemma('good.n.03.goodness') [Lemma('bad.n.01.badness')]\n",
      "Lemma('commodity.n.01.commodity') []\n",
      "Lemma('commodity.n.01.trade_good') []\n",
      "Lemma('commodity.n.01.good') []\n",
      "Lemma('good.a.01.good') [Lemma('bad.a.01.bad')]\n",
      "Lemma('full.s.06.full') []\n",
      "Lemma('full.s.06.good') []\n",
      "Lemma('good.a.03.good') [Lemma('evil.a.01.evil')]\n",
      "Lemma('estimable.s.02.estimable') []\n",
      "Lemma('estimable.s.02.good') []\n",
      "Lemma('estimable.s.02.honorable') []\n",
      "Lemma('estimable.s.02.respectable') []\n",
      "Lemma('beneficial.s.01.beneficial') []\n",
      "Lemma('beneficial.s.01.good') []\n",
      "Lemma('good.s.06.good') []\n",
      "Lemma('good.s.07.good') []\n",
      "Lemma('good.s.07.just') []\n",
      "Lemma('good.s.07.upright') []\n",
      "Lemma('adept.s.01.adept') []\n",
      "Lemma('adept.s.01.expert') []\n",
      "Lemma('adept.s.01.good') []\n",
      "Lemma('adept.s.01.practiced') []\n",
      "Lemma('adept.s.01.proficient') []\n",
      "Lemma('adept.s.01.skillful') []\n",
      "Lemma('adept.s.01.skilful') []\n",
      "Lemma('good.s.09.good') []\n",
      "Lemma('dear.s.02.dear') []\n",
      "Lemma('dear.s.02.good') []\n",
      "Lemma('dear.s.02.near') []\n",
      "Lemma('dependable.s.04.dependable') []\n",
      "Lemma('dependable.s.04.good') []\n",
      "Lemma('dependable.s.04.safe') []\n",
      "Lemma('dependable.s.04.secure') []\n",
      "Lemma('good.s.12.good') []\n",
      "Lemma('good.s.12.right') []\n",
      "Lemma('good.s.12.ripe') []\n",
      "Lemma('good.s.13.good') []\n",
      "Lemma('good.s.13.well') []\n",
      "Lemma('effective.s.04.effective') []\n",
      "Lemma('effective.s.04.good') []\n",
      "Lemma('effective.s.04.in_effect') []\n",
      "Lemma('effective.s.04.in_force') []\n",
      "Lemma('good.s.15.good') []\n",
      "Lemma('good.s.16.good') []\n",
      "Lemma('good.s.16.serious') []\n",
      "Lemma('good.s.17.good') []\n",
      "Lemma('good.s.17.sound') []\n",
      "Lemma('good.s.18.good') []\n",
      "Lemma('good.s.18.salutary') []\n",
      "Lemma('good.s.19.good') []\n",
      "Lemma('good.s.19.honest') []\n",
      "Lemma('good.s.20.good') []\n",
      "Lemma('good.s.20.undecomposed') []\n",
      "Lemma('good.s.20.unspoiled') []\n",
      "Lemma('good.s.20.unspoilt') []\n",
      "Lemma('good.s.21.good') []\n",
      "Lemma('well.r.01.well') [Lemma('ill.r.01.ill')]\n",
      "Lemma('well.r.01.good') []\n",
      "Lemma('thoroughly.r.02.thoroughly') []\n",
      "Lemma('thoroughly.r.02.soundly') []\n",
      "Lemma('thoroughly.r.02.good') []\n"
     ]
    }
   ],
   "source": [
    "#ex2\n",
    "antonyms = []\n",
    "for ant in wordnet.synsets('Good'):   #synset because for every syn it genrates antonym\n",
    "    for lemma in ant.lemmas():\n",
    "        print(lemma , lemma.antonyms())\n",
    "        #few are empty ie its not genrating any antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e3e3dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('good.n.02.good') [Lemma('evil.n.03.evil')]\n",
      "Lemma('good.n.02.goodness') [Lemma('evil.n.03.evilness')]\n",
      "Lemma('good.n.03.good') [Lemma('bad.n.01.bad')]\n",
      "Lemma('good.n.03.goodness') [Lemma('bad.n.01.badness')]\n",
      "Lemma('good.a.01.good') [Lemma('bad.a.01.bad')]\n",
      "Lemma('good.a.03.good') [Lemma('evil.a.01.evil')]\n",
      "Lemma('well.r.01.well') [Lemma('ill.r.01.ill')]\n"
     ]
    }
   ],
   "source": [
    "#to print only the values that arent null \n",
    "antonyms = []\n",
    "for ant in wordnet.synsets('good'):   #synset because for every syn it genrates antonym\n",
    "    for lemma in ant.lemmas():\n",
    "        if lemma.antonyms():  #if it has value then print \n",
    "            print(lemma , lemma.antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ef68344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evil\n",
      "evilness\n",
      "bad\n",
      "badness\n",
      "bad\n",
      "evil\n",
      "ill\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "#to print all antonyms in list format\n",
    "antonyms = []\n",
    "for ant in wordnet.synsets('Good'):\n",
    "    for lemma in ant.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            print(lemma.antonyms()[0].name())\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
